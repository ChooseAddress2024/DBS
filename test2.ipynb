{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Performing Named Entity Recognition...\n",
      "‚úÖ NER results saved to: D:\\Interview_Prep\\DBS\\extracted_entities_with_text.csv\n",
      "üîÅ Grouping similar entities...\n",
      "üéØ Pipeline complete!\n",
      "üìÅ Outputs:\n",
      "   - extracted_entities_with_text.csv\n",
      "   - disambiguated_people.csv\n",
      "   - disambiguated_organizations.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from fuzzywuzzy import fuzz\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# === Step 1: Load spaCy model ===\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except OSError:\n",
    "    print(\"‚ùå spaCy model not found. Run: python -m spacy download en_core_web_sm\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# === Step 2: Load raw CSV input ===\n",
    "input_file = \"D:\\\\Interview_Prep\\\\DBS\\\\financial_crime_news_last_day.csv\"\n",
    "if not os.path.exists(input_file):\n",
    "    print(f\"‚ùå File not found: {input_file}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# === Step 3: Perform NER ===\n",
    "print(\"üîç Performing Named Entity Recognition...\")\n",
    "\n",
    "extracted = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    content = str(row.get(\"contents\", \"\"))\n",
    "    doc = nlp(content)\n",
    "\n",
    "    people = list(set(ent.text.strip() for ent in doc.ents if ent.label_ == \"PERSON\"))\n",
    "    orgs = list(set(ent.text.strip() for ent in doc.ents if ent.label_ == \"ORG\"))\n",
    "\n",
    "    extracted.append({\n",
    "        \"title\": row.get(\"title\", \"\"),\n",
    "        \"contents\": content,\n",
    "        \"published_date\": row.get(\"published_date\", \"\"),\n",
    "        \"people\": people,\n",
    "        \"organizations\": orgs,\n",
    "        \"url\": row.get(\"url\", \"\")\n",
    "    })\n",
    "\n",
    "ner_df = pd.DataFrame(extracted)\n",
    "ner_output = \"D:\\\\Interview_Prep\\\\DBS\\\\extracted_entities_with_text.csv\"\n",
    "ner_df.to_csv(ner_output, index=False)\n",
    "print(f\"‚úÖ NER results saved to: {ner_output}\")\n",
    "\n",
    "# === Step 4: Utility to flatten entity columns safely ===\n",
    "def safely_flatten_column(col):\n",
    "    values = []\n",
    "    for item in col.dropna():\n",
    "        if isinstance(item, list):\n",
    "            values.extend(item)\n",
    "        elif isinstance(item, str):\n",
    "            try:\n",
    "                evaluated = eval(item)\n",
    "                if isinstance(evaluated, list):\n",
    "                    values.extend(evaluated)\n",
    "            except:\n",
    "                continue\n",
    "    return values\n",
    "\n",
    "all_people = safely_flatten_column(ner_df[\"people\"])\n",
    "all_orgs = safely_flatten_column(ner_df[\"organizations\"])\n",
    "\n",
    "# === Step 5: Group similar entities ===\n",
    "def group_similar_entities(entities, threshold=85):\n",
    "    entities = sorted(set(entities))\n",
    "    groups = []\n",
    "    seen = set()\n",
    "\n",
    "    for i, name in enumerate(entities):\n",
    "        if name in seen:\n",
    "            continue\n",
    "        group = [name]\n",
    "        seen.add(name)\n",
    "        for j in range(i + 1, len(entities)):\n",
    "            if entities[j] not in seen and fuzz.token_sort_ratio(name, entities[j]) >= threshold:\n",
    "                group.append(entities[j])\n",
    "                seen.add(entities[j])\n",
    "        groups.append(group)\n",
    "    \n",
    "    return groups\n",
    "\n",
    "def clusters_to_df(clusters, label):\n",
    "    return pd.DataFrame({\n",
    "        \"cluster_id\": [f\"{label}_{i+1}\" for i in range(len(clusters))],\n",
    "        \"entities\": [\", \".join(group) for group in clusters],\n",
    "        \"count\": [len(group) for group in clusters]\n",
    "    })\n",
    "\n",
    "# === Step 6: Cluster people and orgs ===\n",
    "print(\"üîÅ Grouping similar entities...\")\n",
    "\n",
    "people_clusters = group_similar_entities(all_people)\n",
    "org_clusters = group_similar_entities(all_orgs)\n",
    "\n",
    "people_df = clusters_to_df(people_clusters, \"PERSON\")\n",
    "orgs_df = clusters_to_df(org_clusters, \"ORG\")\n",
    "\n",
    "# === Step 7: Save results ===\n",
    "people_df.to_csv(\"D:\\\\Interview_Prep\\\\DBS\\\\disambiguated_people.csv\", index=False)\n",
    "orgs_df.to_csv(\"D:\\\\Interview_Prep\\\\DBS\\\\disambiguated_organizations.csv\", index=False)\n",
    "\n",
    "print(\"üéØ Pipeline complete!\")\n",
    "print(\"üìÅ Outputs:\")\n",
    "print(\"   - extracted_entities_with_text.csv\")\n",
    "print(\"   - disambiguated_people.csv\")\n",
    "print(\"   - disambiguated_organizations.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
